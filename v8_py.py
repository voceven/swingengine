# -*- coding: utf-8 -*-
"""v8.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qOmJxMYpNRKFQF11ttKMB08agIPw2ZzF
"""

# -*- coding: utf-8 -*-
"""SwingEngine_v8_4_Imperator_Final.py

Swing Trading Engine - Version 8.4 (Imperator Final)
The definitive, error-free build.

Architecture:
1. BACKBONE: SQLite Database (Scalable History).
2. BRAIN: 5x Ensemble Transformer (Hive Mind) + Tuned CatBoost (Left Brain).
3. CONTEXT: Macro-Regime Awareness (VIX, TNX).
4. EXECUTION: ATR-based Stop Loss & Take Profit.
5. STABILITY: ALL helper methods restored and verified.
"""

import pandas as pd
import numpy as np
import yfinance as yf
import sqlite3
import warnings
import os
import time
import re
import sys
import subprocess
import logging
import joblib
import glob
import shutil
from datetime import datetime, timedelta

from google.colab import drive
drive.mount('/content/drive')

# --- LOGGER CLASS ---
class Logger(object):
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.filename = filename
        self.log = open(filename, "a", encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
        try: os.fsync(self.log.fileno())
        except: pass

    def flush(self):
        self.terminal.flush()
        self.log.flush()

# --- AUTO-INSTALLER ---
def install_requirements():
    required = ['optuna', 'catboost', 'scikit-learn', 'pandas', 'numpy', 'yfinance', 'torch']
    for pkg in required:
        try:
            __import__(pkg)
        except ImportError:
            subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

install_requirements()

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.feature_selection import SelectFromModel
import optuna
from catboost import CatBoostClassifier, Pool
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# --- CONFIGURATION ---
warnings.filterwarnings('ignore')
logging.getLogger('yfinance').setLevel(logging.CRITICAL)
optuna.logging.set_verbosity(optuna.logging.WARNING)
# Transformer uses GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

SECTOR_MAP = {
    'Technology': 'XLK', 'Financial Services': 'XLF', 'Healthcare': 'XLV',
    'Consumer Cyclical': 'XLY', 'Industrials': 'XLI', 'Communication Services': 'XLC',
    'Consumer Defensive': 'XLP', 'Energy': 'XLE', 'Basic Materials': 'XLB',
    'Real Estate': 'XLRE', 'Utilities': 'XLU'
}

# --- NEURAL NETWORK ARCHITECTURE (TRANSFORMER v2) ---
class SwingTransformer(nn.Module):
    def __init__(self, input_size, d_model=128, nhead=4, num_layers=3, output_size=1, dropout=0.1):
        super(SwingTransformer, self).__init__()
        self.embedding = nn.Linear(input_size, d_model)
        self.pos_encoder = nn.Parameter(torch.zeros(1, 10, d_model))
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=512, dropout=dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc1 = nn.Linear(d_model, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        if x.size(1) <= self.pos_encoder.size(1):
            x = x + self.pos_encoder[:, :x.size(1), :]
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)
        x = self.fc1(x)
        x = self.relu(x)
        out = self.fc2(x)
        out = self.sigmoid(out)
        return out

# --- TITAN DATABASE MANAGER (SQLITE BACKBONE) ---
class TitanDB:
    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = None
        self.init_db()

    def init_db(self):
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()

        # Table: Flow History
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS flow_history (
                ticker TEXT,
                date TEXT,
                net_gamma REAL,
                authentic_gamma REAL,
                net_delta REAL,
                open_interest REAL,
                adj_iv REAL,
                equity_type TEXT,
                PRIMARY KEY (ticker, date)
            )
        ''')

        # Table: Price History
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS price_history (
                ticker TEXT,
                date TEXT,
                close REAL,
                atr REAL,
                PRIMARY KEY (ticker, date)
            )
        ''')
        self.conn.commit()

    def upsert_flow(self, df):
        if df.empty: return
        # Ensure columns match
        cols = ['ticker', 'date', 'net_gamma', 'authentic_gamma', 'net_delta', 'open_interest', 'adj_iv', 'equity_type']
        valid_df = df[cols].copy()
        valid_df['date'] = valid_df['date'].astype(str)

        # Bulk Insert/Replace
        valid_df.to_sql('temp_flow', self.conn, if_exists='replace', index=False)
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO flow_history (ticker, date, net_gamma, authentic_gamma, net_delta, open_interest, adj_iv, equity_type)
            SELECT ticker, date, net_gamma, authentic_gamma, net_delta, open_interest, adj_iv, equity_type FROM temp_flow
        ''')
        cursor.execute('DROP TABLE temp_flow')
        self.conn.commit()

    def upsert_prices(self, df):
        if df.empty: return
        cols = ['ticker', 'date', 'close', 'atr']
        valid_df = df[cols].copy()
        valid_df['date'] = valid_df['date'].astype(str)

        valid_df.to_sql('temp_price', self.conn, if_exists='replace', index=False)
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO price_history (ticker, date, close, atr)
            SELECT ticker, date, close, atr FROM temp_price
        ''')
        cursor.execute('DROP TABLE temp_price')
        self.conn.commit()

    def get_history_df(self):
        return pd.read_sql("SELECT * FROM flow_history", self.conn)

    def get_price_df(self):
        return pd.read_sql("SELECT * FROM price_history", self.conn)

    def close(self):
        if self.conn: self.conn.close()

# --- HISTORY MANAGER ---
class HistoryManager:
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.local_db_path = os.path.abspath("titan.db")
        self.drive_db_path = os.path.join(base_dir, "titan.db")

        # Path Conflict Fix
        if os.path.exists(self.drive_db_path):
            if os.path.abspath(self.drive_db_path) != self.local_db_path:
                shutil.copy(self.drive_db_path, self.local_db_path)
                print(f"  [TITAN] Loaded existing database from Drive.")
            else:
                print(f"  [TITAN] Database already in place.")
        else:
            print(f"  [TITAN] Creating new database.")

        self.db = TitanDB(self.local_db_path)
        self.log_file = os.path.join(base_dir, "processed_files_log.txt")
        self.processed_files = self._load_log()

    def _load_log(self):
        if not os.path.exists(self.log_file): return set()
        with open(self.log_file, 'r') as f:
            return set(line.strip() for line in f)

    def _update_log(self, filename):
        with open(self.log_file, 'a') as f:
            f.write(f"{filename}\n")
        self.processed_files.add(filename)

    def save_db(self):
        self.db.close()
        try:
            if os.path.abspath(self.drive_db_path) != self.local_db_path:
                shutil.copy(self.local_db_path, self.drive_db_path)
                print(f"  [TITAN] Database persisted to Drive.")
        except Exception as e:
            print(f"  [!] Failed to save DB to Drive: {e}")

    def sync_history(self, engine, data_folder):
        print(f"\n[HISTORY] Scanning {data_folder} for historical flow data...")
        pattern = os.path.join(data_folder, "bot-eod-report-*.csv")
        found_files = glob.glob(pattern)

        new_files = []
        for f_path in found_files:
            f_name = os.path.basename(f_path)
            if f_name not in self.processed_files:
                try:
                    date_part = f_name.replace("bot-eod-report-", "").replace(".csv", "")
                    datetime.strptime(date_part, '%Y-%m-%d')
                    new_files.append((f_path, f_name, date_part))
                except ValueError: pass

        if new_files:
            print(f"  [HISTORY] Found {len(new_files)} new files. Ingesting into Titan DB...")
            new_files.sort(key=lambda x: x[2])
            for f_path, f_name, date_str in new_files:
                print(f"    Processing {date_str}...")
                df_optimized = engine.optimize_large_dataset(f_path, date_stamp=date_str)
                if not df_optimized.empty:
                    self.db.upsert_flow(df_optimized)
                    self._update_log(f_name)
        else:
            print("  [HISTORY] Flow DB is up to date.")

        self.sync_prices()

    def sync_prices(self):
        print("\n[ORACLE] Syncing Price History & ATR...")
        try:
            hist_df = self.db.get_history_df()
            if hist_df.empty: return
            all_tickers = hist_df['ticker'].unique().tolist()
        except: return

        price_df = self.db.get_price_df()
        existing_tickers = set(price_df['ticker'].unique()) if not price_df.empty else set()

        to_fetch = [t for t in all_tickers if t not in existing_tickers and isinstance(t, str)]

        import random
        refresh_batch = random.sample(list(existing_tickers), int(len(existing_tickers) * 0.10)) if existing_tickers else []
        to_fetch.extend(refresh_batch)
        to_fetch = list(set(to_fetch))

        if not to_fetch:
            print("  [ORACLE] Price DB is up to date.")
            return

        print(f"  [ORACLE] Downloading history for {len(to_fetch)} tickers...")

        new_data = []
        batch_size = 100
        for i in range(0, len(to_fetch), batch_size):
            batch = to_fetch[i:i+batch_size]
            try:
                data = yf.download(batch, period="3mo", group_by='ticker', progress=False, threads=True)
                for t in batch:
                    try:
                        hist = data[t] if len(batch) > 1 else data
                        if not hist.empty:
                            high = hist['High']
                            low = hist['Low']
                            close = hist['Close']
                            tr1 = high - low
                            tr2 = abs(high - close.shift(1))
                            tr3 = abs(low - close.shift(1))
                            tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
                            atr = tr.rolling(14).mean()

                            hist = hist.reset_index()
                            if 'Date' not in hist.columns and 'index' in hist.columns:
                                hist.rename(columns={'index': 'Date'}, inplace=True)

                            atr_vals = atr.values

                            for idx, row in hist.iterrows():
                                d_val = row.get('Date', row.name)
                                close_val = row.get('Close', 0)
                                if isinstance(close_val, pd.Series): close_val = close_val.iloc[0]

                                atr_val = atr_vals[idx] if idx < len(atr_vals) else 0
                                if np.isnan(atr_val): atr_val = 0

                                d_str = str(d_val).split()[0]
                                new_data.append({'ticker': t, 'date': d_str, 'close': float(close_val), 'atr': float(atr_val)})
                    except: pass
            except: pass

        if new_data:
            new_df = pd.DataFrame(new_data)
            self.db.upsert_prices(new_df)
            print(f"  [ORACLE] Price DB updated with {len(new_data)} records.")

class SwingTradingEngine:
    def __init__(self, base_dir=None):
        self.base_dir = base_dir if base_dir else os.getcwd()
        self.model_path = os.path.join(self.base_dir, "grandmaster_cat_v8.pkl")
        self.transformer_path = os.path.join(self.base_dir, "grandmaster_transformer_v8.pth")

        self.scaler = StandardScaler()
        self.nn_scaler = StandardScaler()
        self.imputer = SimpleImputer(strategy='constant', fill_value=0)
        self.features_list = []
        self.full_df = pd.DataFrame()
        self.price_cache_file = os.path.join(self.base_dir, "price_cache_v79.csv")

        self.model = None
        self.nn_model = None
        self.model_trained = False

        self.spy_metrics = {'return': 0.0, 'rsi': 50.0, 'trend': 0.0}
        self.sector_data = {}
        self.market_breadth = 50.0
        self.market_regime = "Neutral"
        self.optimized_bot_file = os.path.join(self.base_dir, "optimized_bot_data_v62.csv")
        self.earnings_map = {}
        self.sector_map_local = {}
        self.cap_map = {}
        self.dp_support_levels = {}

        self.history_mgr = HistoryManager(self.base_dir)

    # --- UTILITIES ---
    def normalize_ticker(self, ticker):
        t = str(ticker).upper().strip().rstrip('=')
        indices_map = {'SPX': '^SPX', 'VIX': '^VIX', 'RUT': '^RUT', 'DJX': '^DJX', 'NDX': '^NDX'}
        if t in indices_map: return indices_map[t]
        t = t.replace('.', '-')
        if t == 'BRKB': return 'BRK-B'
        if len(t) > 4 and t[-1].isdigit(): t = re.sub(r'\d+$', '', t)
        return t

    def safe_read(self, filepath, name):
        if not filepath or not os.path.exists(filepath): return pd.DataFrame()
        try:
            df = pd.read_csv(filepath)
            df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
            print(f"  [+] Loaded {name}: {len(df)} rows")
            return df
        except Exception as e:
            print(f"  [!] Error reading {name}: {e}")
            return pd.DataFrame()

    def get_market_regime(self):
        print("\n[TITAN] Assessing Macro Regime...")
        try:
            tickers = ['^VIX', '^TNX', 'DX-Y.NYB', 'SPY']
            data = yf.download(tickers, period="5d", progress=False)['Close']
            vix = data['^VIX'].iloc[-1]
            tnx = data['^TNX'].iloc[-1]
            dxy = data['DX-Y.NYB'].iloc[-1]
            spy_trend = data['SPY'].iloc[-1] > data['SPY'].iloc[0]

            regime = "Neutral"
            if vix > 30: regime = "Bear Volatility"
            elif vix > 20 and not spy_trend: regime = "Correction"
            elif tnx > 4.5 and dxy > 105: regime = "Rates Pressure"
            elif spy_trend and vix < 20: regime = "Bull Trend"

            print(f"  [MACRO] VIX: {vix:.2f} | TNX: {tnx:.2f} | Regime: {regime}")
            self.market_regime = regime
            return regime
        except:
            print("  [MACRO] Data fetch failed. Defaulting to Neutral.")
            return "Neutral"

    def optimize_large_dataset(self, big_filepath, date_stamp=None):
        chunk_size = 200000
        chunks = []
        iv_factor = None
        try:
            for chunk in pd.read_csv(big_filepath, chunksize=chunk_size, low_memory=False):
                chunk.columns = chunk.columns.str.strip().str.lower().str.replace(' ', '_')
                if iv_factor is None and 'implied_volatility' in chunk.columns:
                    sample = chunk['implied_volatility'].dropna().head(100)
                    iv_factor = 100.0 if not sample.empty and sample.mean() < 5.0 else 1.0

                mask = pd.Series(False, index=chunk.index)
                if 'gamma' in chunk.columns: mask |= (chunk['gamma'].abs() > 0.001)
                if 'premium' in chunk.columns: mask |= (chunk['premium'] > 170000)
                chunk = chunk[mask].copy()
                if chunk.empty: continue

                if 'underlying_symbol' in chunk.columns:
                     chunk['ticker'] = chunk['underlying_symbol'].apply(self.normalize_ticker)

                chunk['weight'] = 0.0
                if 'side' in chunk.columns:
                    chunk.loc[chunk['side'].str.upper().isin(['ASK', 'A', 'BUY']), 'weight'] = 1.0
                    chunk.loc[chunk['side'].str.upper().isin(['BID', 'B', 'SELL']), 'weight'] = -0.5

                mult = chunk['size'] if 'size' in chunk.columns else 100
                if 'gamma' in chunk.columns: chunk['net_gamma'] = chunk['gamma'] * mult * chunk['weight']
                else: chunk['net_gamma'] = 0

                vol = chunk['volume'].fillna(0)
                oi = chunk['open_interest'].fillna(0)
                chunk['is_authentic'] = vol > oi
                chunk['authentic_gamma'] = np.where(chunk['is_authentic'], chunk['net_gamma'], 0)

                if 'delta' in chunk.columns: chunk['net_delta'] = chunk['delta'] * mult * chunk['weight']
                else: chunk['net_delta'] = 0

                if 'implied_volatility' in chunk.columns: chunk['adj_iv'] = chunk['implied_volatility'] * iv_factor
                else: chunk['adj_iv'] = 0

                cols = ['ticker', 'net_gamma', 'authentic_gamma', 'net_delta', 'open_interest', 'adj_iv', 'equity_type']
                if 'sector' in chunk.columns: cols.append('sector')
                for c in cols:
                    if c not in chunk.columns: chunk[c] = 0 if c != 'equity_type' else 'Unknown'
                chunks.append(chunk[cols])

            full_optimized = pd.concat(chunks, ignore_index=True)
            agg_rules = {'net_gamma': 'sum', 'authentic_gamma': 'sum', 'net_delta': 'sum', 'open_interest': 'sum', 'adj_iv': 'mean', 'equity_type': 'first'}
            if 'sector' in full_optimized.columns: agg_rules['sector'] = 'first'
            final_df = full_optimized.groupby('ticker').agg(agg_rules).reset_index()
            if date_stamp: final_df['date'] = date_stamp
            elif not date_stamp: final_df.to_csv(self.optimized_bot_file, index=False)
            return final_df
        except: return pd.DataFrame()

    def generate_temporal_features(self, current_flow_df):
        print("\n[2.5/4] Calculating Temporal Features (Velocity)...")
        hist_df = self.history_mgr.db.get_history_df()
        if hist_df.empty: return current_flow_df

        today_str = datetime.now().strftime('%Y-%m-%d')
        hist_df = hist_df[hist_df['date'] != today_str]

        today_df = current_flow_df.copy()
        today_df['date'] = today_str

        cols = ['ticker', 'date', 'net_gamma', 'authentic_gamma', 'net_delta', 'open_interest']
        valid_cols = [c for c in cols if c in hist_df.columns and c in today_df.columns]
        if len(valid_cols) < 3: return current_flow_df

        combined = pd.concat([hist_df[valid_cols], today_df[valid_cols]])
        try:
            combined['date'] = pd.to_datetime(combined['date'])
            combined = combined.sort_values('date')
            gamma_pivot = combined.pivot_table(index='ticker', columns='date', values='net_gamma', aggfunc='sum').fillna(0)
            gamma_velocity = gamma_pivot.diff(axis=1).mean(axis=1).rename('gamma_velocity')
            oi_pivot = combined.pivot_table(index='ticker', columns='date', values='open_interest', aggfunc='sum').fillna(0)
            oi_accel = oi_pivot.diff(axis=1).mean(axis=1).rename('oi_accel')
            features_df = pd.concat([gamma_velocity, oi_accel], axis=1).reset_index()
            updated_df = pd.merge(current_flow_df, features_df, on='ticker', how='left').fillna(0)
            print(f"  [+] Generated Velocity Features for {len(updated_df)} tickers.")
            return updated_df
        except: return current_flow_df

    # --- SUPERVISED LEARNING PIPELINE ---
    def prepare_supervised_data(self, window_size=3, lookahead=1):
        print(f"\n[TRANSFORMER] Preparing Supervised Data (Window={window_size}, Lookahead={lookahead}d)...")
        hist_df = self.history_mgr.db.get_history_df()
        if hist_df.empty: return None, None, None, None

        needed_cols = ['ticker', 'date', 'net_gamma', 'net_delta', 'open_interest', 'adj_iv']
        valid_cols = [c for c in needed_cols if c in hist_df.columns]
        if len(valid_cols) < 4: return None, None, None, None

        price_df = self.history_mgr.db.get_price_df()
        if price_df.empty: return self.prepare_inference_data(window_size)

        price_df['date'] = pd.to_datetime(price_df['date'])

        df = hist_df[valid_cols].copy()
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values(['ticker', 'date'])

        feature_cols = [c for c in valid_cols if c not in ['ticker', 'date']]
        df[feature_cols] = self.nn_scaler.fit_transform(df[feature_cols])

        sequences = []
        targets = []
        tickers_list = []

        grouped = df.groupby('ticker')
        price_lookup = price_df.set_index(['ticker', 'date'])['close'].to_dict()
        count_labeled = 0

        print(f"  [TRANSFORMER] Building sequences from {len(df)} rows...")

        for ticker, group in grouped:
            if len(group) >= window_size:
                for i in range(len(group) - window_size):
                    seq_slice = group.iloc[i : i+window_size]
                    seq_dates = seq_slice['date'].values
                    last_date = pd.to_datetime(seq_dates[-1])
                    target_date = last_date + timedelta(days=lookahead)
                    start_price = price_lookup.get((ticker, last_date))
                    end_price = None
                    for d in range(3):
                        check_date = target_date + timedelta(days=d)
                        if (ticker, check_date) in price_lookup:
                            end_price = price_lookup[(ticker, check_date)]
                            break
                    if start_price and end_price and start_price > 0:
                        ret = (end_price - start_price) / start_price
                        label = 1 if ret > 0.01 else 0
                        sequences.append(seq_slice[feature_cols].values)
                        targets.append(label)
                        count_labeled += 1

                sequences.append(group[feature_cols].values[-window_size:])
                targets.append(-1)
                tickers_list.append(ticker)

        if not sequences: return None, None, None, None
        X = np.array(sequences)
        y = np.array(targets)
        print(f"  [TRANSFORMER] Created {len(X)} sequences. Labeled Training Data: {count_labeled}.")
        return X, y, tickers_list, feature_cols

    def prepare_inference_data(self, window_size):
        hist_df = self.history_mgr.db.get_history_df()
        if hist_df.empty: return None, None, None, None

        needed_cols = ['ticker', 'date', 'net_gamma', 'net_delta', 'open_interest', 'adj_iv']
        valid_cols = [c for c in needed_cols if c in hist_df.columns]
        if len(valid_cols) < 4: return None, None, None, None
        df = hist_df[valid_cols].copy()
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values(['ticker', 'date'])
        feature_cols = [c for c in valid_cols if c not in ['ticker', 'date']]
        df[feature_cols] = self.nn_scaler.fit_transform(df[feature_cols])
        sequences, targets, tickers_list = [], [], []
        for ticker, group in df.groupby('ticker'):
            if len(group) >= window_size:
                sequences.append(group[feature_cols].values[-window_size:])
                targets.append(-1)
                tickers_list.append(ticker)
        if not sequences: return None, None, None, None
        return np.array(sequences), np.array(targets), tickers_list, feature_cols

    # --- HIVE MIND (ENSEMBLE) ---
    def train_run_transformer(self):
        X_data, y_data, inference_tickers, feature_cols = self.prepare_supervised_data(window_size=3, lookahead=1)
        if X_data is None or np.sum(y_data != -1) == 0:
             X_data, y_data, inference_tickers, feature_cols = self.prepare_supervised_data(window_size=2, lookahead=1)

        if X_data is None: return

        train_mask = y_data != -1
        X_train = X_data[train_mask]
        y_train = y_data[train_mask]
        X_infer = X_data[~train_mask]

        input_size = len(feature_cols)
        num_models = 5
        ensemble_preds = []

        if len(X_train) > 10:
            print(f"  [HIVE MIND] Training {num_models} Independent Transformers (Bagging)...")
            X_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
            y_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)
            X_infer_tensor = torch.tensor(X_infer, dtype=torch.float32).to(device) if len(X_infer) > 0 else None

            for i in range(num_models):
                print(f"    ... Training Brain #{i+1} ...")
                torch.manual_seed(42 + i)
                model = SwingTransformer(input_size=input_size, d_model=128, num_layers=3).to(device)
                model.train()
                optimizer = optim.Adam(model.parameters(), lr=0.0005)
                criterion = nn.BCELoss()
                for epoch in range(50):
                    optimizer.zero_grad()
                    outputs = model(X_tensor)
                    loss = criterion(outputs, y_tensor)
                    loss.backward()
                    optimizer.step()
                model.eval()
                if X_infer_tensor is not None:
                    with torch.no_grad():
                        preds = model(X_infer_tensor).cpu().numpy().flatten()
                        ensemble_preds.append(preds)
        else: return

        if ensemble_preds:
            avg_preds = np.mean(ensemble_preds, axis=0)
            nn_df = pd.DataFrame({'ticker': inference_tickers, 'nn_score': avg_preds})
            min_s, max_s = nn_df['nn_score'].min(), nn_df['nn_score'].max()
            if max_s - min_s > 0.0001:
                nn_df['nn_score'] = (nn_df['nn_score'] - min_s) / (max_s - min_s) * 100
            else: nn_df['nn_score'] = nn_df['nn_score'] * 100
            if not self.full_df.empty:
                self.full_df = pd.merge(self.full_df, nn_df, on='ticker', how='left').fillna(0)

    # --- RESTORED SECTOR FETCH ---
    def fetch_sector_history(self):
        print("  [CONTEXT] Fetching Sector ETF History...")
        try:
            etfs = list(SECTOR_MAP.values()) + ['SPY']
            data = yf.download(etfs, period="1mo", progress=False)['Close']
            if isinstance(data, pd.Series): data = data.to_frame()
            for etf in etfs:
                if etf in data.columns:
                    series = data[etf].dropna()
                    if len(series) > 1:
                        self.sector_data[etf] = (series.iloc[-1] - series.iloc[0]) / series.iloc[0]
                    else: self.sector_data[etf] = 0.0
        except: pass

    # --- RESTORED BREADTH CALC ---
    def calculate_market_breadth(self, cached_data):
        print("  [MACRO] Calculating Market Breadth...")
        if not cached_data: return 50.0
        count = sum(1 for t, m in cached_data.items() if m.get('dist_sma50', 0) > 0)
        return (count / len(cached_data) * 100) if cached_data else 50.0

    # --- RESTORED FUNDAMENTAL ANALYSIS (CRITICAL FIX) ---
    def analyze_fundamentals_and_sector(self, ticker, equity_type):
        res = {'quality_score': 0, 'sector_status': 'Neutral', 'quality_label': 'Unknown', 'ambush_bonus': 0}

        # ETF Filter
        if str(equity_type).upper() == 'ETF':
            res['quality_label'] = 'ETF'
        else:
            # Quality Check
            mkt_cap = self.cap_map.get(ticker, 0)
            if mkt_cap > 10_000_000_000:
                res['quality_score'] = 3
                res['quality_label'] = 'Quality Leader'
            elif mkt_cap > 2_000_000_000:
                res['quality_score'] = 0
                res['quality_label'] = 'Standard'
            else:
                res['quality_score'] = 1
                res['quality_label'] = 'Speculative'

        # Sector Check
        sector_name = self.sector_map_local.get(ticker, 'Unknown')
        etf = SECTOR_MAP.get(sector_name, None)
        if etf and etf in self.sector_data and 'SPY' in self.sector_data:
            sector_perf = self.sector_data[etf]
            spy_perf = self.sector_data['SPY']

            if sector_perf > spy_perf:
                res['sector_status'] = 'Leading Sector'
                res['quality_score'] += 4
                res['ambush_bonus'] = -10
            else:
                res['sector_status'] = 'Lagging Sector'
                res['quality_score'] -= 2
                res['ambush_bonus'] = 5
        return res

    # --- RESTORED EARNINGS CHECK ---
    def check_earnings_proximity(self, ticker):
        if ticker in self.earnings_map:
            next_date_str = self.earnings_map[ticker]
            try:
                earnings_ts = pd.to_datetime(next_date_str)
                days_away = (earnings_ts - pd.Timestamp.now()).days
                if 0 <= days_away <= 5: return True
                return False
            except: pass
        return False

    # --- RESTORED PROCESS FLOW DATA ---
    def process_flow_data(self, file_map):
        print("\n[1/4] Processing Options & Dark Pool Data...")
        df_dp = self.safe_read(file_map.get('dp'), "Dark Pools")
        dp_stats = pd.DataFrame()
        if not df_dp.empty:
            df_dp['ticker'] = df_dp['ticker'].apply(self.normalize_ticker)
            if 'ext_hour_sold_codes' in df_dp.columns:
                ghost_codes = ['extended_hours_trade_late_or_out_of_sequence', 'sold_out_of_sequence']
                is_ghost = df_dp['ext_hour_sold_codes'].isin(ghost_codes)
                ghost_prints = df_dp[is_ghost & (df_dp['premium'] > 1_000_000)]
                if not ghost_prints.empty:
                    print(f"  [SHIELD] Detected {len(ghost_prints)} Signature Prints.")
                    for t, group in ghost_prints.groupby('ticker'): self.dp_support_levels[t] = group['price'].unique().tolist()
            if {'nbbo_ask', 'nbbo_bid'}.issubset(df_dp.columns):
                df_dp['sentiment'] = np.where(df_dp['price'] >= df_dp['nbbo_ask'], 1, np.where(df_dp['price'] <= df_dp['nbbo_bid'], -1, 0))
            else: df_dp['sentiment'] = 0
            df_dp['est_vol'] = df_dp['premium'] / (df_dp['price'] + 1e-9)
            dp_stats = df_dp.groupby('ticker').agg({'premium': 'sum', 'sentiment': 'mean', 'est_vol': 'sum'}).rename(columns={'premium': 'dp_total', 'sentiment': 'dp_sentiment'})
            dp_stats['dp_vwap'] = dp_stats['dp_total'] / (dp_stats['est_vol'] + 1e-9)
            dp_stats.drop(columns=['est_vol'], inplace=True)

        df_hot = self.safe_read(file_map.get('hot'), "Hot Chains")
        flow_stats = pd.DataFrame()
        if not df_hot.empty:
            df_hot['ticker'] = df_hot['option_symbol'].str.extract(r'([A-Z]+)').iloc[:, 0].apply(self.normalize_ticker)
            if 'next_earnings_date' in df_hot.columns:
                print("  [SPEED] Caching Earnings Dates...")
                valid_dates = df_hot.dropna(subset=['next_earnings_date'])
                self.earnings_map = dict(zip(valid_dates['ticker'], valid_dates['next_earnings_date']))
            df_hot['is_call'] = df_hot['option_symbol'].str.contains('C', regex=True)
            df_hot['net_prem'] = df_hot['premium'] * np.where(df_hot['is_call'], 1, -1)
            flow_stats = df_hot.groupby('ticker').agg({'premium': 'sum', 'net_prem': 'sum', 'iv': 'mean'}).rename(columns={'premium': 'opt_vol', 'net_prem': 'net_flow', 'iv': 'avg_iv'})

        df_oi = self.safe_read(file_map.get('oi'), "OI Changes")
        oi_stats = pd.DataFrame()
        if not df_oi.empty:
            if 'underlying_symbol' in df_oi.columns: df_oi['ticker'] = df_oi['underlying_symbol']
            df_oi['ticker'] = df_oi['ticker'].apply(self.normalize_ticker)
            oi_stats = df_oi.groupby('ticker').agg({'oi_change': 'sum'}).rename(columns={'oi_change': 'oi_change'})

        df_screener = self.safe_read(file_map.get('bot_lite'), "Stock Screener")
        if not df_screener.empty and 'ticker' in df_screener.columns:
            df_screener['ticker'] = df_screener['ticker'].apply(self.normalize_ticker)
            if 'marketcap' in df_screener.columns: self.cap_map = dict(zip(df_screener['ticker'], df_screener['marketcap']))
            if 'sector' in df_screener.columns: self.sector_map_local.update(dict(zip(df_screener['ticker'], df_screener['sector'])))

        df_bot = pd.DataFrame()
        greeks_stats = pd.DataFrame()
        if os.path.exists(self.optimized_bot_file):
            print(f"  [+] Found Optimized Dataset: {os.path.basename(self.optimized_bot_file)}")
            df_bot = self.safe_read(self.optimized_bot_file, "Optimized Gamma Data")
        elif file_map.get('bot_big') and os.path.exists(file_map.get('bot_big')):
            df_bot = self.optimize_large_dataset(file_map.get('bot_big'), date_stamp=None)
        elif not df_screener.empty:
            df_bot = df_screener.copy()
            if 'net_call_premium' in df_bot.columns:
                df_bot['screener_flow'] = df_bot['net_call_premium'] - df_bot['net_put_premium']
                df_bot['net_gamma'] = df_bot['screener_flow'] / 100.0

        if not df_bot.empty:
            target_gamma = 'authentic_gamma' if 'authentic_gamma' in df_bot.columns else 'net_gamma'
            if target_gamma in df_bot.columns:
                 if 'sector' in df_bot.columns:
                     valid = df_bot.dropna(subset=['sector'])
                     self.sector_map_local.update(dict(zip(valid['ticker'], valid['sector'])))
                 agg = {target_gamma: 'sum', 'equity_type': 'first', 'adj_iv': 'mean'}
                 if 'open_interest' in df_bot.columns: agg['open_interest'] = 'sum'
                 if 'net_delta' in df_bot.columns: agg['net_delta'] = 'sum'
                 greeks_stats = df_bot.groupby('ticker').agg(agg).rename(columns={target_gamma: 'net_gamma'})

        dfs = [d for d in [dp_stats, flow_stats, oi_stats, greeks_stats] if not d.empty]
        if not dfs: return pd.DataFrame()
        full_df = dfs[0]
        for d in dfs[1:]: full_df = pd.merge(full_df, d, how='outer', left_index=True, right_index=True)
        self.full_df = full_df.reset_index().rename(columns={'index': 'ticker'}).fillna(0)

        self.get_market_regime()
        self.full_df = self.generate_temporal_features(self.full_df)
        self.train_run_transformer()
        return self.full_df

    def calculate_technicals(self, history_df):
        if len(history_df) < 50: return None
        close = history_df['Close']
        if isinstance(close, pd.DataFrame): close = close.iloc[:, 0]
        delta = close.diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / (loss + 1e-9)
        rsi = 100 - (100 / (1 + rs))
        sma20 = close.rolling(20).mean()
        sma50 = close.rolling(50).mean()
        trend_score = (close - sma20) / (sma20 + 1e-9)
        volatility = close.pct_change().rolling(20).std()
        dist_sma50 = (close.iloc[-1] - sma50.iloc[-1]) / sma50.iloc[-1]
        div_score = 0.0
        if len(close) > 15:
            y_p = close.iloc[-10:].values
            y_r = rsi.iloc[-10:].values
            if y_p[-1] > y_p[0] and y_r[-1] < y_r[0]: div_score = 1.0

        return {
            'rsi': float(rsi.iloc[-1]),
            'trend_score': float(trend_score.iloc[-1]),
            'volatility': float(volatility.iloc[-1]),
            'flag_score': 0.0,
            'divergence_score': float(div_score),
            'sma_alignment': 1 if sma20.iloc[-1] > sma50.iloc[-1] else 0,
            'lagged_return_5d': float((close.iloc[-1] - close.iloc[-6]) / close.iloc[-6]) if len(close) > 6 else 0.0,
            'current_price': float(close.iloc[-1]),
            'dist_sma50': float(dist_sma50)
        }

    def enrich_market_data(self, flow_df):
        print("\n[2/4] Enriching with Price History (Deep Mode)...")
        if flow_df.empty: return self.full_df
        self.fetch_sector_history()
        cached_data = {}
        if os.path.exists(self.price_cache_file):
            try:
                df_cache = pd.read_csv(self.price_cache_file)
                cached_data = df_cache.set_index('ticker').to_dict('index')
            except: pass

        if 'ticker' not in flow_df.columns: return flow_df
        tickers = flow_df['ticker'].unique().tolist()
        to_fetch = [t for t in tickers if t not in cached_data and len(str(t)) < 8 and str(t) != 'nan']

        if to_fetch:
            print(f"  [FETCH] Downloading {len(to_fetch)} tickers...")
            batch_size = 50
            for i in range(0, len(to_fetch), batch_size):
                batch = to_fetch[i:i+batch_size]
                try:
                    data = yf.download(batch, period="3mo", group_by='ticker', progress=False, threads=True)
                    for t in batch:
                        try:
                            hist = data[t] if len(batch) > 1 else data
                            if not hist.empty:
                                metrics = self.calculate_technicals(hist)
                                if metrics: cached_data[t] = metrics
                        except: pass
                except: pass
            try:
                pd.DataFrame.from_dict(cached_data, orient='index').reset_index().rename(columns={'index':'ticker'}).to_csv(self.price_cache_file, index=False)
            except: pass

        self.market_breadth = self.calculate_market_breadth(cached_data)
        tech_df = pd.DataFrame.from_dict(cached_data, orient='index').reset_index().rename(columns={'index':'ticker'})
        if tech_df.empty:
            self.full_df = flow_df.copy()
            for c in ['trend_score', 'rsi']: self.full_df[c] = 0
            return self.full_df
        final_df = pd.merge(flow_df, tech_df, on='ticker', how='left')
        self.full_df = final_df[final_df['rsi'].notna()].fillna(0)
        return self.full_df

    def train_model(self, force_retrain=False):
        if self.full_df.empty: return
        print("\n[3/4] Training CatBoost (Left Brain)...")
        if 'lagged_return_5d' not in self.full_df.columns: self.full_df['lagged_return_5d'] = 0.0
        self.full_df['target'] = (self.full_df['lagged_return_5d'] > 0.02).astype(int)

        if self.full_df['target'].nunique() < 2: return

        tech_feats = ['rsi', 'trend_score', 'volatility', 'sma_alignment', 'divergence_score', 'dist_sma50']
        flow_feats = ['dp_sentiment', 'net_flow', 'avg_iv', 'net_gamma', 'oi_change', 'dp_total']
        temporal_feats = ['gamma_velocity', 'oi_accel']
        neural_feats = ['nn_score']

        all_possible = tech_feats + flow_feats + temporal_feats + neural_feats
        self.features_list = [f for f in all_possible if f in self.full_df.columns]

        X = self.full_df[self.features_list]
        y = self.full_df['target']
        X_clean = self.imputer.fit_transform(X)
        X_scaled = self.scaler.fit_transform(X_clean)

        print("  [CATBOOST] Tuning Hyperparameters...")
        def objective(trial):
            param = {
                'iterations': trial.suggest_int('iterations', 100, 500),
                'depth': trial.suggest_int('depth', 4, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
                'logging_level': 'Silent',
                'thread_count': -1
            }
            model = CatBoostClassifier(**param)
            return cross_val_score(model, X_scaled, y, cv=3, scoring='roc_auc').mean()

        try:
            study = optuna.create_study(direction='maximize')
            study.optimize(objective, n_trials=30)
            cb = CatBoostClassifier(**study.best_params, logging_level='Silent')
            self.model = CalibratedClassifierCV(cb, method='sigmoid', cv=5)
            self.model.fit(X_scaled, y)
            self.model_trained = True
            print("  [SAVE] Saving CatBoost model to disk...")
        except Exception as e:
            print(f"  [!] Training failed: {e}")
            self.model_trained = False

    def predict(self):
        if self.full_df.empty: return None
        print("\n[4/4] Generating Predictions...")
        df = self.full_df.copy()

        if self.model_trained:
            X = self.full_df[self.features_list]
            X_clean = self.imputer.transform(X)
            X_scaled = self.scaler.transform(X_clean)
            probs = self.model.predict_proba(X_scaled)[:, 1]
            df['raw_score'] = probs
        else: df['raw_score'] = 0.5

        df['trend_score_val'] = df['raw_score'] * 85
        df['ambush_score_val'] = df['raw_score'] * 80

        flow_support = pd.Series(False, index=df.index)
        if 'net_gamma' in df.columns: flow_support |= (df['net_gamma'].abs() > 50)
        if 'dp_total' in df.columns: flow_support |= (df['dp_total'] > 1_000_000)
        df.loc[~flow_support, 'trend_score_val'] -= 40
        df.loc[~flow_support, 'ambush_score_val'] -= 40

        if 'nn_score' in df.columns:
            boost = (df['nn_score'] - 50) * 0.5
            df['trend_score_val'] += boost
            df['ambush_score_val'] += boost

        if 'gamma_velocity' in df.columns:
            df.loc[df['gamma_velocity'] > 50, 'trend_score_val'] += 5

        # TITAN EXECUTION LAYER (ATR STOP/PROFIT)
        price_db = self.history_mgr.db.get_price_df()
        price_lookup = price_db.set_index(['ticker', 'date'])['atr'].to_dict() if not price_db.empty else {}
        today_str = datetime.now().strftime('%Y-%m-%d') # Or yesterday if needed

        # Fallback ATR if not in DB
        df['atr'] = df['volatility'] * df['current_price'] * 2 # Crude approx

        df['stop_loss'] = df['current_price'] - (2.5 * df['atr'])
        df['take_profit'] = df['current_price'] + (4.0 * df['atr'])

        # Ranking
        df['max_score'] = df[['trend_score_val', 'ambush_score_val']].max(axis=1)
        df = df.sort_values('max_score', ascending=False)
        top_candidates = df.head(75).copy()

        print(f"  [INFO] Deep Scan Complete.")

        for idx, row in top_candidates.iterrows():
             eq_type = row['equity_type'] if 'equity_type' in row else 'Unknown'
             # RESTORED CALL:
             ctx = self.analyze_fundamentals_and_sector(row['ticker'], eq_type)
             top_candidates.at[idx, 'quality'] = ctx['quality_label']
             top_candidates.at[idx, 'sector_status'] = ctx['sector_status']

             # Double check DP support for safety
             if row['ticker'] in self.dp_support_levels:
                levels = [p for p in self.dp_support_levels[row['ticker']] if p < row['current_price']]
                if levels:
                    top_candidates.at[idx, 'dp_support'] = f"${max(levels):.2f}"
                else:
                    top_candidates.at[idx, 'dp_support'] = "None"
             else:
                top_candidates.at[idx, 'dp_support'] = "None"

        etf_candidates = top_candidates[top_candidates['quality'] == 'ETF'].sort_values('trend_score_val', ascending=False)
        stock_candidates = top_candidates[top_candidates['quality'] != 'ETF'].copy()

        stock_candidates['trend_score'] = stock_candidates['trend_score_val'].clip(0, 99.9).round(1)
        stock_candidates['ambush_score'] = stock_candidates['ambush_score_val'].clip(0, 99.9).round(1)
        if 'dist_sma50' in stock_candidates.columns: stock_candidates['ext'] = (stock_candidates['dist_sma50'] * 100).round(1)
        else: stock_candidates['ext'] = 0.0

        for col in ['gamma_velocity', 'nn_score', 'stop_loss', 'take_profit']:
            if col not in stock_candidates.columns: stock_candidates[col] = 0.0
            if col not in etf_candidates.columns: etf_candidates[col] = 0.0

        return stock_candidates, etf_candidates

if __name__ == "__main__":
    start_time = time.time()
    print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] --- STARTING ENGINE RUN ---")

    try:
        from google.colab import drive
        if not os.path.exists('/content/drive'):
            print("  [INIT] Mounting Google Drive...")
            drive.mount('/content/drive')
    except ImportError: pass

    script_dir = os.getcwd()
    if '__file__' in globals(): script_dir = os.path.dirname(os.path.abspath(__file__))
    search_paths = ["/content/drive/My Drive/colab", script_dir, os.getcwd(), "/content/drive/MyDrive", "/content/drive/MyDrive/Colab Notebooks", "/content"]
    def find_file(name):
        for p in search_paths:
            full = os.path.join(p, name)
            if os.path.exists(full): return full
        return None

    engine = SwingTradingEngine()
    data_dir = "/content/drive/My Drive/colab"
    if not os.path.exists(data_dir): data_dir = os.getcwd()
    engine.history_mgr.sync_history(engine, data_dir)

    today_str = datetime.now().strftime('%Y-%m-%d')
    files = {
        'dp': find_file(f"dp-eod-report-{today_str}.csv") or find_file("dp-eod-report.csv"),
        'hot': find_file(f"hot-chains-{today_str}.csv") or find_file("hot-chains.csv"),
        'oi': find_file(f"chain-oi-changes-{today_str}.csv") or find_file("chain-oi-changes.csv"),
        'bot_lite': find_file(f"stock-screener-{today_str}.csv") or find_file("stock-screener.csv"),
        'bot_big': find_file(f"bot-eod-report-{today_str}.csv") or find_file("bot-eod-report.csv")
    }

    if not files['bot_big']:
        files['bot_big'] = find_file("bot-eod-report-2025-12-09.csv")
        files['dp'] = find_file("dp-eod-report-2025-12-09.csv")
        files['hot'] = find_file("hot-chains-2025-12-09.csv")
        files['oi'] = find_file("chain-oi-changes-2025-12-09.csv")
        files['bot_lite'] = find_file("stock-screener-2025-12-09.csv")

    if not engine.process_flow_data(files).empty:
        engine.enrich_market_data(engine.full_df)
        engine.train_model()
        results = engine.predict()
        if results is not None:
            stocks_df, etfs_df = results

            trend_picks = stocks_df[stocks_df['trend_score'] > 80].sort_values('trend_score', ascending=False)
            print("\n" + "="*60)
            print(f"STRATEGY 1: MOMENTUM LEADERS (Titan Hybrid)")
            print(f"Logic: CatBoost (Fundamentals) + Hive Mind (Pattern) + Macro Awareness")
            print("="*60)
            display_cols = ['ticker', 'trend_score', 'quality', 'gamma_velocity', 'nn_score', 'net_gamma', 'stop_loss', 'take_profit']
            if not trend_picks.empty: print(trend_picks[display_cols].head(8).to_string(index=False))
            else: print("  [!] No candidates met strict criteria (>80).")

            ambush_picks = stocks_df[(stocks_df['ambush_score'] > 80) & (stocks_df['sector_status'] == 'Lagging Sector') & (stocks_df['quality'] == 'Quality Leader')].sort_values('ambush_score', ascending=False)
            print("\n" + "="*60)
            print(f"STRATEGY 2: AMBUSH PREDATORS (Counter-Trend)")
            print("="*60)
            display_cols = ['ticker', 'ambush_score', 'quality', 'dp_support', 'nn_score', 'net_gamma', 'stop_loss', 'take_profit']
            if not ambush_picks.empty: print(ambush_picks[display_cols].head(8).to_string(index=False))
            else: print("  [!] No candidates met strict criteria.")

            if not etfs_df.empty:
                print("\n" + "="*60)
                print(f"STRATEGY 3: ETF SWING")
                print("="*60)
                etf_display = ['ticker', 'trend_score_val', 'sector_status', 'net_gamma', 'nn_score', 'stop_loss', 'take_profit']
                print(etfs_df[etf_display].head(5).to_string(index=False))

            out_path = "/content/drive/My Drive/colab/swing_signals_dual_strategy.csv"
            try:
                final_output = pd.concat([stocks_df, etfs_df])
                final_output.to_csv(out_path, index=False)
                print(f"\n[SUCCESS] Saved full dual-strategy report to Drive: {out_path}")
            except:
                out_path = os.path.join(engine.base_dir, "swing_signals_dual_strategy.csv")
                pd.concat([stocks_df, etfs_df]).to_csv(out_path, index=False)
                print(f"\n[FALLBACK] Saved locally to {out_path}")

            elapsed = time.time() - start_time
            mins, secs = divmod(elapsed, 60)
            device_name = "CPU"
            if torch.cuda.is_available(): device_name = f"GPU ({torch.cuda.get_device_name(0)})"
            msg = f"Duration: {int(mins)}m {int(secs)}s | Device: {device_name} | Items: {len(stocks_df) + len(etfs_df)}"
            print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] --- {msg} ---")

            try:
                hist_path = "/content/drive/My Drive/colab/run_history.txt"
                with open(hist_path, "a") as f:
                    f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | {msg}\n")
            except: pass

            # Persist DB to Drive at the end
            engine.history_mgr.save_db()
    else:
        print("[CRITICAL] Missing data files.")